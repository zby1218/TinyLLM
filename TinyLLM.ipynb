{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6a3f549a-5be0-4f7b-b993-e9bbd95bd349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import os\n",
    "from torch.utils.data import IterableDataset, Dataset\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import  PreTrainedModel\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from transformers import PretrainedConfig\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM, AutoTokenizer, DefaultDataCollator, DataCollatorForTokenClassification, AutoConfig\n",
    "from datasets import load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07733afb-d5f9-43b7-896b-daf5e12b8aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846414e9-5d83-45f6-bb91-b1851ea27555",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"TinyLLM pretrain\", name=\"batch_size=16 lr=2e-4 max_step=25000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b404d5-2294-45e8-830f-26bc34255793",
   "metadata": {},
   "source": [
    "RMSNorm公式为:\n",
    "\n",
    "$ RMSNorm(x) = \\frac{x}{RMS(x)} \\cdot  \\gamma $  \n",
    "$ RMS(x) = \\sqrt{\\frac{1}{d} \\sum_{i=1}^d x_i^2} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b88ea9db-561f-4c28-b773-33213eb20800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSNorm\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        super().__init__()\n",
    "        # gemma 增大特征表达能力\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        # 防止分母为 0 \n",
    "        self.variance_epsilon = eps\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = hidden_states.float()\n",
    "        variance = hidden_states.pow(2).mean(-1,keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states.float()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38dd519d-5639-4269-93ae-1a601bc10c51",
   "metadata": {},
   "source": [
    "旋转后的矩阵为\n",
    "\n",
    "$v' = R(\\theta) \\cdot v = \\begin{bmatrix} v_1 \\cdot \\cos(\\theta) - v_2 \\cdot \\sin(\\theta) \\\\ v_1 \\cdot \\sin(\\theta) + v_2 \\cdot \\cos(\\theta)\\end{bmatrix}$\n",
    "\n",
    "公式可写为\n",
    "$v' = R(\\theta) \\cdot v = v \\cdot \\cos(\\theta) + \\text{rotate\\_half}(v) \\cdot \\sin(\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae6064ca-290a-401d-9e1c-97041a846ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoPE\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x.chunk(2, dim = -1)\n",
    "    return torch.cat((-x2, x1), dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a143bb0-d169-42cb-b19b-3060f5a85157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 应用RoPE\n",
    "def apply_RoPE(q, k, cos, sin, unsqueeze_dim = 2):\n",
    "    # 增加维度以对 GQA 的Q K shape进行广播\n",
    "    cos = cos.unsqueeze(unsqueeze_dim) # (1, seq_length, d_model) -> (1, seq_length, 1, d_model)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim) \n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin) # (batch_size, seq_length, head_num, d_model) =  (batch_size, seq_length, head_num, d_model) * (1, seq_length, 1, d_model) 广播\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f15c3cbb-36c5-4671-bc61-8df6e6fc41f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPE(nn.Module):\n",
    "    def __init__(self, dim, max_seq_length = 2048):\n",
    "        super(RoPE, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.max_seq_length = max_seq_length\n",
    "        # 绝对位置信息\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        t = torch.arange(max_seq_length).float().unsqueeze(1) # (max_seq_length, 1)\n",
    "        freqs = t @ inv_freq.unsqueeze(0)  #(max_seq_len, 1) * (1, dim/2) = (max_seq_len, dim/2)\n",
    "        freqs = torch.cat((freqs, freqs), dim=-1)  # (max_seq_len, dim)\n",
    "        \n",
    "        self.register_buffer(\"cos_cached\", freqs.cos())\n",
    "        self.register_buffer(\"sin_cached\", freqs.sin())\n",
    "    def forward(self, q, k):\n",
    "        # 根据seq_length截取 freqs 得到sin cos数值\n",
    "        cos = self.cos_cached[:q.shape[1], :].unsqueeze(0) # (1, seq_length, dim)\n",
    "        sin = self.sin_cached[:q.shape[1], :].unsqueeze(0)\n",
    "        return apply_RoPE(q, k, cos, sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c22babe7-5e3c-4fbe-9437-1677761cf4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group Query Attention(GQA)需要一个Q共享 多个K,V 此函数对K V进行复制\n",
    "# param1: hidden_state ,param2: n_rep 复制次数\n",
    "def repeat_kv(hidden_states, n_rep):\n",
    "    batch, seq_length, head_num, d_k = hidden_states.shape\n",
    "    # 复制一次则不动\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, :, None, :].expand(batch, seq_length, head_num, n_rep, d_k)\n",
    "    return hidden_states.reshape(batch, seq_length, head_num * n_rep, d_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f66f75fc-9639-4ce5-911d-201e3d9e6649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention类\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.dropout = config.dropout\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.head_num = config.attention_head_num\n",
    "        self.head_dim = getattr(config, \"head_dim\", self.hidden_size // self.head_num)\n",
    "        self.kvhead_num = config.kvhead_num\n",
    "        self.kvgroup_num = self.head_num // self.kvhead_num\n",
    "        self.k_cache, self.v_cache = None, None\n",
    "        self.is_causal = True\n",
    "        self.flash_attn = self.config.flash_attn\n",
    "\n",
    "        # 初始化矩阵\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.head_num * self.head_dim, bias = config.attention_bias)\n",
    "        # GQA KV分组\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.kvhead_num * self.head_dim, bias = config.attention_bias)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.kvhead_num * self.head_dim, bias = config.attention_bias)\n",
    "        self.o_proj = nn.Linear(self.head_num * self.head_dim, self.hidden_size, bias = config.attention_bias)\n",
    "        self.residual_dropout = nn.Dropout(self.dropout)\n",
    "        self.attention_dropout = nn.Dropout(self.dropout)\n",
    "        self.RoPE_emb = RoPE(self.head_dim)\n",
    "\n",
    "    def forward(self, hidden_states, use_kv_cache = False):\n",
    "        batch, seq_length = hidden_states[:2]\n",
    "\n",
    "        if use_kv_cache and self.eval():\n",
    "            if self.k_cache is None or self.k_cache.shape[1] != seq_length - 1:\n",
    "                q, k, v = self.q_proj(hidden_states), self.k_proj(hidden_states), self.v_proj(hidden_states)\n",
    "            else:\n",
    "                # 获取最新生成的token\n",
    "                token = hidden_states[:, -1:, :] # 形状(b, 1, dim)\n",
    "                q = torch.cat((torch.zeros_like(hidden_states[:, :-1, :]), self.q_proj(token)), dim=1) \n",
    "                # 新的k,v和之前已经生成的进行拼接\n",
    "                k = torch.cat((self.k_cache, self.k_proj(token)), dim=1)\n",
    "                v = torch.cat((self.v_cache, self.v_proj(token)), dim=1)\n",
    "            # 更新cache\n",
    "            self.k_cache, self.v_cache = k, v\n",
    "            \n",
    "        else:\n",
    "            q, k, v = self.q_proj(hidden_states), self.k_proj(hidden_states), self.v_proj(hidden_states)\n",
    "        \n",
    "        q = q.view(batch, seq_length, self.head_num, self.head_dim) # (batch, seq_length, head_num, head_dim/d_k)\n",
    "        k = k.view(batch, seq_length, self.kvhead_num, self.head_dim)\n",
    "        v = v.view(batch, seq_length, self.kvhead_num, self.head_dim)\n",
    "\n",
    "        q, k = self.RoPE_emb(q, k)\n",
    "\n",
    "        k = repeat_kv(k, self.kvgroup_num)\n",
    "        v = repeat_kv(v, self.kvgroup_num)\n",
    "\n",
    "        q = q.transpose(1,2)\n",
    "        k = k.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "\n",
    "        if self.flash_attn:\n",
    "            output = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p = self.dropout if self.training else 0.0, is_causal = self.is_causal)\n",
    "        else:\n",
    "            mask = torch.full((1, 1, self.config.max_seq_length, self.config.max_seq_length), float(\"-inf\"))\n",
    "            mask = torch.triu(mask, diagonal = 1)\n",
    "            scores = torch.matmul(q, k.transpose(2, 3))/math.sqrt(self.head_dim)\n",
    "            scores = scores + self.mask[:, :, :seq_length, :seq_length]\n",
    "            scores = F.softmax(scores.float(), dim = -1).type_as(q)\n",
    "            scores = self.attention_dropout(scores)\n",
    "            output = torch.matmul(scores, v)\n",
    "        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch, seq_length, -1)\n",
    "        output = self.o_proj(output)\n",
    "        output = self.residual_dropout(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "edfcd209-7b62-46ac-8b0d-6c2a0e27d5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_proj = self.down_proj(F.silu(self.gate_proj(x)) * self.up_proj(x))\n",
    "        return down_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bb0e3ced-8263-4f2f-9137-47d84d12ce5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, config, layer_idx):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.self_attn = Attention(config)\n",
    "        self.mlp = MLP(config)\n",
    "        self.input_layernorm = RMSNorm(config.hidden_size)\n",
    "        self.post_attention_layernorm = RMSNorm(config.hidden_size)\n",
    "        self.layer_idx = layer_idx\n",
    "    def forward(self, hidden_states, use_kv_cache):\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "        hidden_states = self.self_attn(hidden_states=hidden_states, use_kv_cache=use_kv_cache)\n",
    "        \n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        outputs = hidden_states\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1fc1010d-fe31-4033-aa21-ae9e3ca6e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(PretrainedConfig):\n",
    "    model_type = \"small_model\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                hidden_size = 512,\n",
    "                attention_head_num = 16,\n",
    "                kvhead_num = 8,\n",
    "                flash_attn = True,\n",
    "                attention_bias = False,\n",
    "                max_seq_len = 512,\n",
    "                intermediate_size = 2048,\n",
    "                mlp_bias = False,\n",
    "                vocab_size = 6400,\n",
    "                n_layers = 8,\n",
    "                dropout = 0.0,\n",
    "                **kwargs):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_head_num = attention_head_num\n",
    "        self.kvhead_num = kvhead_num\n",
    "        self.flash_attn = flash_attn\n",
    "        self.attention_bias = attention_bias\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.mlp_bias = mlp_bias\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        super().__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed34268c-1a5a-45e2-b9c1-ea0f928133ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM(PreTrainedModel):\n",
    "    config_class = Config\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.vocab_size = self.config.vocab_size\n",
    "        self.n_layers = self.config.n_layers\n",
    "\n",
    "        self.tokon_embeddings = nn.Embedding(self.config.vocab_size, self.config.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.config.dropout) \n",
    "        self.layers = torch.nn.ModuleList() \n",
    "        for layer_idx in range(self.n_layers):\n",
    "            self.layers.append(DecoderLayer(self.config, layer_idx)) \n",
    "        self.norm = RMSNorm(self.config.hidden_size)\n",
    "        self.output = nn.Linear(self.config.hidden_size, self.config.vocab_size, bias=False) \n",
    "        self.apply(self._init_weights) \n",
    "        self.loss = None \n",
    "        \n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('w3.weight') or pn.endswith('wo.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layers)) \n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)  \n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)  \n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02) \n",
    "            \n",
    "        \n",
    "    def forward(self, input_ids, labels, use_kv_cache=False):\n",
    "       \n",
    "        hidden_states = self.tokon_embeddings(input_ids) \n",
    "        hidden_states = self.dropout(hidden_states)  \n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            hidden_states = layer(hidden_states, use_kv_cache=use_kv_cache)  \n",
    "\n",
    "        hidden_states = self.norm(hidden_states) \n",
    "\n",
    "        if labels is not None:\n",
    "            logits = self.output(hidden_states)  \n",
    "            self.loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=0) \n",
    "        else:\n",
    "            logits = self.output(hidden_states[:, [-1], :])  \n",
    "            self.loss = None  \n",
    "\n",
    "        return CausalLMOutputWithPast(self.loss, logits)\n",
    "    \n",
    "    @torch.inference_mode\n",
    "    def generate(self, inputs, eos, max_new_tokens, temperature=0.7, top_k=None, stream=True, repetition_penalty=1., use_kv_cache=True):\n",
    "        \n",
    "        input_ids = inputs['input_ids']\n",
    "        labels = inputs['labels']\n",
    "        s = input_ids.shape[1]\n",
    "        while input_ids.shape[1] < max_new_tokens - 1:  \n",
    "            inference_res = self(input_ids, labels, use_kv_cache=use_kv_cache)  \n",
    "            logits = inference_res.logits \n",
    "            # 取最后一个token的logits\n",
    "            logits = logits[:, -1, :] \n",
    "\n",
    "            for token in set(input_ids.tolist()[0]):  \n",
    "                logits[:, token] /= repetition_penalty\n",
    "\n",
    "            if temperature == 0.0: \n",
    "                _, idx_next = torch.topk(logits, k=1, dim=-1)\n",
    "            else:\n",
    "                logits = logits / temperature  \n",
    "                if top_k is not None:  \n",
    "                    v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                    logits[logits < v[:, [-1]]] = -float('Inf') \n",
    "\n",
    "                probs = F.softmax(logits, dim=-1)  \n",
    "                idx_next = torch.multinomial(probs, num_samples=1, generator=None)  \n",
    "\n",
    "            if idx_next == eos:  \n",
    "                break\n",
    "\n",
    "            input_ids = torch.cat((input_ids, idx_next), dim=1)  \n",
    "            if stream:  \n",
    "                yield input_ids[:, s:]  \n",
    "\n",
    "        if not stream:  \n",
    "            yield input_ids[:, s:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "218ff2db-b1ce-448c-ae0a-1fb14e39661d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "model = LLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5ba4fa-f6a5-4c1c-b375-83b1596478eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'模型参数量为：{sum(p.numel() for p in model.parameters() if p.requires_grad)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "816916da-67a9-4619-b726-eadeca1ee2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = ''\n",
    "tokenizer_path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "62d23ac5-4431-4c25-8806-eb759148f82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "dataset = load_dataset('json', data_files='autodl-tmp/mobvoi_seq_monkey_general_open_corpus.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487be13a-1b87-4a78-82bc-1fd3d837bffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(dataset, tokenizer, max_seq_len):\n",
    "    processed_data = []\n",
    "    for example in tqdm(dataset, desc=\"Processing dataset\"):\n",
    "        text = f\"<s>{example['text']}</s>\"\n",
    "\n",
    "        # 对文本进行编码\n",
    "        input_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "        text_len = len(input_ids)\n",
    "\n",
    "        # 限制长度\n",
    "        if text_len > max_seq_len:\n",
    "            input_ids = input_ids[:max_seq_len]\n",
    "        else:\n",
    "            input_ids = input_ids + [0] * (max_seq_len - text_len)\n",
    "\n",
    "        # 构造 input_ids 和 labels\n",
    "        input_ids = np.array(input_ids)\n",
    "        X = torch.tensor(input_ids[:-1], dtype=torch.long)  # 输入\n",
    "        Y = torch.tensor(input_ids[1:], dtype=torch.long)  # 标签\n",
    "\n",
    "        processed_data.append({\n",
    "            'input_ids': X,\n",
    "            'labels': Y\n",
    "        })\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cb353a-fd63-4e28-b46b-5fd1ac5709f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = process_data(dataset['train'],tokenizer,config.max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fbad89b3-c6a4-4dbd-a01b-264be99c6b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DefaultDataCollator()\n",
    "args = TrainingArguments(output_dir='./results', \n",
    "                        num_train_epochs=20, \n",
    "                        do_train=True, \n",
    "                        per_device_train_batch_size=16,\n",
    "                        gradient_accumulation_steps=8,\n",
    "                        group_by_length=False,\n",
    "                        max_steps=10,\n",
    "                        logging_steps=10,\n",
    "                        report_to = 'wandb')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5567cd3d-a041-46c0-9e6d-41a0b16eadbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, args=args, train_dataset=dataset, tokenizer=tokenizer, data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aee3031-9490-47f4-95d7-e2be69e9fad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
