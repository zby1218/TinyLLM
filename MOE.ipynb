{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d9a572-2437-4370-ab5e-981f0bb3b449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import IterableDataset, Dataset\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import  PreTrainedModel\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from transformers import PretrainedConfig\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM, AutoTokenizer, DefaultDataCollator, DataCollatorForTokenClassification, AutoConfig\n",
    "from dataset import SFTDataset, LLMDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc123ee1-f85c-4446-a734-03215d52f071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSNorm\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        super().__init__()\n",
    "        # gemma 增大特征表达能力\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        # 防止分母为 0 \n",
    "        self.variance_epsilon = eps\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = hidden_states.float()\n",
    "        variance = hidden_states.pow(2).mean(-1,keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994b66b9-502d-4505-9d60-4dec2058e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoPE\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x.chunk(2, dim = -1)\n",
    "    return torch.cat((-x2, x1), dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e89cfef-f842-41de-bfea-2f4fa479dc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 应用RoPE\n",
    "def apply_RoPE(q, k, cos, sin, unsqueeze_dim = 2):\n",
    "    # 增加维度以对 GQA 的Q K shape进行广播\n",
    "    cos = cos.unsqueeze(unsqueeze_dim) # (1, seq_length, d_model) -> (1, seq_length, 1, d_model)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim) \n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin) # (batch_size, seq_length, head_num, d_model) =  (batch_size, seq_length, head_num, d_model) * (1, seq_length, 1, d_model) 广播\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef1313e-eb93-41b7-a6a7-04ea131d008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPE(nn.Module):\n",
    "    def __init__(self, dim, max_seq_length = 2048):\n",
    "        super(RoPE, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.max_seq_length = max_seq_length\n",
    "        # 绝对位置信息\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        t = torch.arange(max_seq_length).float().unsqueeze(1) # (max_seq_length, 1)\n",
    "        freqs = t @ inv_freq.unsqueeze(0)  #(max_seq_len, 1) * (1, dim/2) = (max_seq_len, dim/2)\n",
    "        freqs = torch.cat((freqs, freqs), dim=-1)  # (max_seq_len, dim)\n",
    "        \n",
    "        self.register_buffer(\"cos_cached\", freqs.cos())\n",
    "        self.register_buffer(\"sin_cached\", freqs.sin())\n",
    "    def forward(self, q, k):\n",
    "        # 根据seq_length截取 freqs 得到sin cos数值\n",
    "        cos = self.cos_cached[:q.shape[1], :].unsqueeze(0) # (1, seq_length, dim)\n",
    "        sin = self.sin_cached[:q.shape[1], :].unsqueeze(0)\n",
    "        return apply_RoPE(q, k, cos, sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6fabf3-4ece-4822-b4d2-cd856978d749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group Query Attention(GQA)需要一个Q共享 多个K,V 此函数对K V进行复制\n",
    "# param1: hidden_state ,param2: n_rep 复制次数\n",
    "def repeat_kv(hidden_states, n_rep):\n",
    "    batch, seq_length, head_num, d_k = hidden_states.shape\n",
    "    # 复制一次则不动\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, :, None, :].expand(batch, seq_length, head_num, n_rep, d_k)\n",
    "    return hidden_states.reshape(batch, seq_length, head_num * n_rep, d_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3f8e23-b960-44e1-9664-70fd691cf3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention类\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.dropout = config.dropout\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.head_num = config.attention_head_num\n",
    "        self.head_dim = getattr(config, \"head_dim\", self.hidden_size // self.head_num)\n",
    "        self.kvhead_num = config.kvhead_num\n",
    "        self.kvgroup_num = self.head_num // self.kvhead_num\n",
    "        self.k_cache, self.v_cache = None, None\n",
    "        self.is_causal = True\n",
    "        self.flash_attn = self.config.flash_attn\n",
    "\n",
    "        # 初始化矩阵\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.head_num * self.head_dim, bias = config.attention_bias)\n",
    "        # GQA KV分组\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.kvhead_num * self.head_dim, bias = config.attention_bias)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.kvhead_num * self.head_dim, bias = config.attention_bias)\n",
    "        self.o_proj = nn.Linear(self.head_num * self.head_dim, self.hidden_size, bias = config.attention_bias)\n",
    "        self.residual_dropout = nn.Dropout(self.dropout)\n",
    "        self.attention_dropout = nn.Dropout(self.dropout)\n",
    "        self.RoPE_emb = RoPE(self.head_dim)\n",
    "\n",
    "    def forward(self, hidden_states, use_kv_cache = False):\n",
    "        batch, seq_length = hidden_states[:2]\n",
    "\n",
    "        if use_kv_cache and self.eval():\n",
    "            if self.k_cache is None or self.k_cache.shape[1] != seq_length - 1:\n",
    "                q, k, v = self.q_proj(hidden_states), self.k_proj(hidden_states), self.v_proj(hidden_states)\n",
    "            else:\n",
    "                # 获取最新生成的token\n",
    "                token = hidden_states[:, -1:, :] # 形状(b, 1, dim)\n",
    "                q = torch.cat((torch.zeros_like(hidden_states[:, :-1, :]), self.q_proj(token)), dim=1) \n",
    "                # 新的k,v和之前已经生成的进行拼接\n",
    "                k = torch.cat((self.k_cache, self.k_proj(token)), dim=1)\n",
    "                v = torch.cat((self.v_cache, self.v_proj(token)), dim=1)\n",
    "            # 更新cache\n",
    "            self.k_cache, self.v_cache = k, v\n",
    "            \n",
    "        else:\n",
    "            q, k, v = self.q_proj(hidden_states), self.k_proj(hidden_states), self.v_proj(hidden_states)\n",
    "        \n",
    "        q = q.view(batch, seq_length, self.head_num, self.head_dim) # (batch, seq_length, head_num, head_dim/d_k)\n",
    "        k = k.view(batch, seq_length, self.kvhead_num, self.head_dim)\n",
    "        v = v.view(batch, seq_length, self.kvhead_num, self.head_dim)\n",
    "\n",
    "        q, k = self.RoPE_emb(q, k)\n",
    "\n",
    "        k = repeat_kv(k, self.kvgroup_num)\n",
    "        v = repeat_kv(v, self.kvgroup_num)\n",
    "\n",
    "        q = q.transpose(1,2)\n",
    "        k = k.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "\n",
    "        if self.flash_attn:\n",
    "            output = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p = self.dropout if self.training else 0.0, is_causal = self.is_causal)\n",
    "        else:\n",
    "            mask = torch.full((1, 1, self.config.max_seq_length, self.config.max_seq_length), float(\"-inf\"))\n",
    "            mask = torch.triu(mask, diagonal = 1)\n",
    "            scores = torch.matmul(q, k.transpose(2, 3))/math.sqrt(self.head_dim)\n",
    "            scores = scores + self.mask[:, :, :seq_length, :seq_length]\n",
    "            scores = F.softmax(scores.float(), dim = -1).type_as(q)\n",
    "            scores = self.attention_dropout(scores)\n",
    "            output = torch.matmul(scores, v)\n",
    "        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch, seq_length, -1)\n",
    "        output = self.o_proj(output)\n",
    "        output = self.residual_dropout(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253bb28d-42f1-4fbd-be1f-8dc4f0ed4f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        down_proj = self.down_proj(F.silu(self.gate_proj(x)) * self.up_proj(x))\n",
    "        return down_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ccfed9-148b-4a76-b850-a070fee73371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_balancing_loss_func(gate_logits, num_experts, top_k):\n",
    "    concatenated_gate_logits = torch.cat([layer_gate for layer_gate in gate_logits], dim=0) # 各个层的gate_logit进行合并[layers X batch_size X sequence_length, num_experts]\n",
    "    routing_weights = F.softmax(concatenated_gate_logits, dim = -1)\n",
    "    _, selected_experts = torch.topk(routing_weights, top_k, dim = -1)\n",
    "    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n",
    "    \n",
    "    tokens_per_expert = torch.mean(expert_mask.float(), dim = 0)\n",
    "\n",
    "\n",
    "    router_prob_per_expert = torch.mean(routing_weights, dim = 0)\n",
    "    overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n",
    "    return overall_loss * num_experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b463c4be-cf62-4ba6-a91f-67b43e831cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gating(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.topk = config.topk\n",
    "        self.expert_num = config.expert_num\n",
    "        self.gate = nn.Linear(self.hidden_size, self.expert_num)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x dim: b, s, hidden_size\n",
    "        logits = self.gate(x)  # gate: b, s, expert_num\n",
    "        logits_topk, indices = logits.topk(self.topk, dim = -1) # 选择概率最大的两个专家，返回两个专家对每个token的概率\n",
    "        zeros = torch.full_like(logits, float(\"-inf\")) # 创建一个全为负无穷的矩阵，用于屏蔽其他专家的概率并重新归一化概率最大的两个专家\n",
    "        sparse_logits = zeros.scatter(dim = -1, index = indices, src = logits_topk)  # 将选择的两个专家的概率按指定索引填充\n",
    "        sparse_logits = F.softmax(sparse_logits, dim = -1) # 得到一个稀疏矩阵，选择的两个专家对每个token的概率和为1\n",
    "        gate_logit = logits.view(-1, self.expert_num)\n",
    "        \n",
    "        return sparse_logits, indices, gate_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a689fa16-26a1-4725-b497-da9261afa0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias) \n",
    "    def forward(self, x):\n",
    "        down_proj = self.down_proj(F.silu(self.gate_proj(x)) * self.up_proj(x))\n",
    "        return down_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6087bcc8-555e-44aa-b7f4-93ebc76062f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoE(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.experts = nn.ModuleList([Expert(config) for _ in range(config.expert_num)])\n",
    "        self.gating = Gating(config)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        sparse_logits, indices, gate_logit = self.gating(x)\n",
    "        final_outputs = torch.zeros_like(x) \n",
    "        x_flat = x.view(-1, x.shape[-1])  # (batch_size * seq_len, dim)\n",
    "        sparse_logits_flat = sparse_logits.view(-1, sparse_logits.shape[-1])  # (batch_size * seq_len, export_num))\n",
    "        \n",
    "        for i, expert in enumerate(self.experts):\n",
    "            expert_mask = (indices == i).any(-1)  # (batch_size, seq_len)\n",
    "            expert_mask_flat = expert_mask.view(-1) # (batch_size * seq_len)\n",
    "            if expert_mask_flat.any():\n",
    "                expert_input = x_flat[expert_mask_flat]  # (seq_true, dim)\n",
    "                export_output = expert(expert_input)  # (seq_true, dim)\n",
    "                \n",
    "                gate_scores = sparse_logits_flat[expert_mask_flat, i].unsqueeze(1)  # (seq_true) --> (seq_true, 1)\n",
    "                \n",
    "                weighted_output = export_output * gate_scores  # (seq_true, dim)\n",
    "                \n",
    "                final_outputs[expert_mask] += weighted_output\n",
    "                \n",
    "        \n",
    "        return final_outputs, gate_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc0da73-b91a-47ff-a1a9-8fea9bf1f46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, config, layer_idx):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.self_attn = Attention(config)\n",
    "        self.moe = MoE(config)\n",
    "        self.mlp = MLP(config)\n",
    "        self.input_layernorm = RMSNorm(config.hidden_size)\n",
    "        self.post_attention_layernorm = RMSNorm(config.hidden_size)\n",
    "        self.layer_idx = layer_idx\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        use_kv_cache\n",
    "    ):\n",
    "        residual = hidden_states\n",
    "\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "        # Self Attention\n",
    "        hidden_states = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            use_kv_cache=use_kv_cache\n",
    "        )\n",
    "        \n",
    "        hidden_states = residual + hidden_states\n",
    "        # Fully Connected\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        if self.layer_idx % 2 == 0:\n",
    "            hidden_states = self.mlp(hidden_states)\n",
    "            gate_logit = None\n",
    "        else:\n",
    "            hidden_states, gate_logit = self.moe(hidden_states)\n",
    "        outputs = residual + hidden_states\n",
    "        return outputs, gate_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71a2b1b-f6df-46ac-a57c-8beab1431b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(PretrainedConfig):\n",
    "    model_type = \"moe_model\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                hidden_size = 512,\n",
    "                attention_head_num = 16,\n",
    "                kvhead_num = 8,\n",
    "                flash_attn = True,\n",
    "                attention_bias = False,\n",
    "                max_seq_len = 512,\n",
    "                intermediate_size = 2048,\n",
    "                mlp_bias = False,\n",
    "                vocab_size = 6400,\n",
    "                n_layers = 8,\n",
    "                dropout = 0.0,\n",
    "                expert_num = 4,\n",
    "                topk = 2,\n",
    "                output_router_logits = True,\n",
    "                aux_loss_coef = 0.01,\n",
    "                **kwargs):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_head_num = attention_head_num\n",
    "        self.kvhead_num = kvhead_num\n",
    "        self.flash_attn = flash_attn\n",
    "        self.attention_bias = attention_bias\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.mlp_bias = mlp_bias\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.expert_num = expert_num\n",
    "        self.topk = topk\n",
    "        self.output_router_logits = output_router_logits\n",
    "        self.aux_loss_coef = aux_loss_coef\n",
    "        super().__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d811834-4645-493e-a247-4eb64d3da71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM(PreTrainedModel):\n",
    "    config_class = Config\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.vocab_size = self.config.vocab_size\n",
    "        self.n_layers = self.config.n_layers\n",
    "        self.expert_num = self.config.expert_num\n",
    "        self.topk = self.config.topk\n",
    "        \n",
    "        self.tokon_embeddings = nn.Embedding(self.config.vocab_size, self.config.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.config.dropout) \n",
    "        self.layers = torch.nn.ModuleList() \n",
    "        for layer_idx in range(self.n_layers):\n",
    "            self.layers.append(DecoderLayer(self.config, layer_idx)) \n",
    "        self.norm = RMSNorm(self.config.hidden_size)\n",
    "        self.output = nn.Linear(self.config.hidden_size, self.config.vocab_size, bias = False) \n",
    "        self.tokon_embeddings.weight = self.output.weight\n",
    "        self.apply(self._init_weights) \n",
    "        self.loss = None \n",
    "        self.aux_loss = None\n",
    "        \n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('w3.weight') or pn.endswith('wo.weight'):\n",
    "                torch.nn.init.normal_(p, mean = 0.0, std = 0.02 / math.sqrt(2 * self.config.n_layers)) \n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std = 0.02)  \n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)  \n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02) \n",
    "            \n",
    "        \n",
    "    def forward(self, input_ids, labels, use_kv_cache=False):\n",
    "        \n",
    "        all_router_logits = () if self.config.output_router_logits else None\n",
    "       \n",
    "        hidden_states = self.tokon_embeddings(input_ids) \n",
    "        hidden_states = self.dropout(hidden_states)  \n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            hidden_states, gate_logit = layer(hidden_states, use_kv_cache = use_kv_cache)\n",
    "            if gate_logit is not None:\n",
    "                all_router_logits += (gate_logit, )  \n",
    "\n",
    "        hidden_states = self.norm(hidden_states) \n",
    "        \n",
    "        \n",
    "        if labels is not None:\n",
    "            logits = self.output(hidden_states)  \n",
    "            self.loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=0) \n",
    "        else:\n",
    "            logits = self.output(hidden_states[:, [-1], :])  \n",
    "            self.loss = None  \n",
    "        \n",
    "        if self.config.output_router_logits:\n",
    "            self.aux_loss = load_balancing_loss_func(all_router_logits, self.expert_num, self.topk)\n",
    "            \n",
    "            if labels is not None:\n",
    "                self.loss += self.config.aux_loss_coef * self.aux_loss.to(self.loss.device)\n",
    "\n",
    "        return CausalLMOutputWithPast(self.loss, logits)\n",
    "    \n",
    "    @torch.inference_mode\n",
    "    def generate(self, inputs, eos, max_new_tokens, temperature=0.7, top_k=None, stream=True, repetition_penalty = 1.,\n",
    "                 use_kv_cache=True):\n",
    "        \n",
    "        input_ids = inputs['input_ids']\n",
    "        labels = inputs['labels']\n",
    "        s = input_ids.shape[1]\n",
    "        while input_ids.shape[1] < max_new_tokens - 1:  \n",
    "            inference_res = self(input_ids, labels, use_kv_cache = use_kv_cache)  \n",
    "            logits = inference_res.logits \n",
    "            logits = logits[:, -1, :] \n",
    "\n",
    "            for token in set(input_ids.tolist()[0]):  \n",
    "                logits[:, token] /= repetition_penalty\n",
    "\n",
    "            if temperature == 0.0: \n",
    "                _, idx_next = torch.topk(logits, k = 1, dim = -1)\n",
    "            else:\n",
    "                logits = logits / temperature  \n",
    "                if top_k is not None:  \n",
    "                    v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                    logits[logits < v[:, [-1]]] = -float('Inf') \n",
    "\n",
    "                probs = F.softmax(logits, dim = -1)  \n",
    "                idx_next = torch.multinomial(probs, num_samples = 1, generator = None)  \n",
    "\n",
    "            if idx_next == eos:  \n",
    "                break\n",
    "\n",
    "            input_ids = torch.cat((input_ids, idx_next), dim = 1)  \n",
    "            if stream:  \n",
    "                yield input_ids[:, s:]  \n",
    "\n",
    "        if not stream:  \n",
    "            yield input_ids[:, s:] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
